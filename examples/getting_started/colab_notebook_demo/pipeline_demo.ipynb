{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5a79557",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NOAA-EPIC/global-eagle/blob/feature/hello_world/examples/getting_started/colab_notebook_demo/pipeline_demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9aa7f4",
   "metadata": {},
   "source": [
    "# Welcome to the `ufs2arco` + `anemoi` + `wxvx` pipeline!\n",
    "\n",
    "Before we start, let's go over a few Google Colab tips!\n",
    "\n",
    "Q) Where are files located?!\n",
    "\n",
    "A) You should see a navigation bar on the left of your screen. The bottom option is a folder. Click on that and you will see all files in your workspace. If you have not run anything yet, you should only see a \"sample data\" folder (this automatically populates in any colab notebook). Throughout this notebook you can go into this area and watch your files populate, look at plots, and edit yamls if you wish to update any configurations on your own. Note: sometimes clicking through folders can be a little laggy.\n",
    "\n",
    "Q) How do I connect to compute?!\n",
    "\n",
    "A) You will need to connect to a runtime. Towards the top right of your screen you will see the words RAM and disk. There is a drop down button next to that. Click there, and then click \"change runtime type\". Make sure \"Python 3\" is selected under runtime type, and if available, select a T4 GPU as your hardware accelerator. If not available, you can run this notebook with a CPU but it will be very, very slow during training. If you happen to have credits for an A-100, use that! All estimated run times in this notebook are from using a T4 GPU.\n",
    "\n",
    "Now that you are connected to compute and know where to find your files, let's do some machine learning!\n",
    "\n",
    "This notebook will guide you through an entire ML pipeline.\n",
    "1) Data preprocessing using `ufs2arco` to create training and validation datasets\n",
    "2) Model training using `anemoi-core` modules to train a graph-based model\n",
    "3) Creating a forecast with `anemoi-inference` to run inference from a model checkpoint\n",
    "4) Verifying your forecast (or multiple!) with `wxvx` to verify against the Global Forecast System (GFS)\n",
    "\n",
    "More information about the various modules and instructions will be provided within each individual step. You will also find additional instructions if you wish to change configurations yourself at all!\n",
    "\n",
    "Acknowledgments:\n",
    "- ufs2arco and Anemoi configurations were adapted from Tim Smith at NOAA Physical Sciences Laboratory\n",
    "    - https://github.com/NOAA-PSL/anemoi-house\n",
    "- ufs2arco: Tim Smith (NOAA Physical Sciences Laboratory)\n",
    "    - https://github.com/NOAA-PSL/ufs2arco\n",
    "- Anemoi: European Centre for Medium-Range Weather Forecasts\n",
    "    - https://github.com/ecmwf/anemoi-core\n",
    "    - https://github.com/ecmwf/anemoi-inference\n",
    "- wxvx: Paul Madden (NOAA Global Systems Laboratory/Cooperative Institute for Research In Environmental Sciences)\n",
    "     - https://github.com/maddenp-cu/wxvx\n",
    "- Notebook authored by Mariah Pope (EPIC/Tomorrow.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0a0dbc",
   "metadata": {},
   "source": [
    "### Step 1: Environment Setup\n",
    "Runtime: 1 minute\n",
    "\n",
    "You will receive a popup after all packages are installed. Click \"restart session\" on the popup and continue on to the next step.\n",
    "\n",
    "You may see some red warnings about imcompatibility because of the numpy version. This is expected behavior and you can ignore this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5490612",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install anemoi-datasets==0.5.25 anemoi-graphs==0.6.2 anemoi-models==0.8.1 anemoi-training==0.5.1 anemoi-inference==0.6.3 trimesh 'numpy<2.3' 'earthkit-data<0.14.0' ufs2arco mpi4py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7bbf06",
   "metadata": {},
   "source": [
    "Clone repository\n",
    "\n",
    "After executing this cell you should see the `global-eagle` repository along with `sample_data` to the left of your screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b04d60d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/NOAA-EPIC/global-eagle.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44e75fb",
   "metadata": {},
   "source": [
    "### Step 2: Create training and validation datasets with ufs2arco\n",
    "\n",
    "Runtime: 5 minutes\n",
    "\n",
    "`ufs2arco` is a python package developed by NOAA Physical Sciences Laboratory (PSL) that is designed to make NOAA forecast, reanalysis, and reforecast datasets more accessible for scientific analysis and machine learning model development. The name stems from its original intent, which was to transform output from the Unified Forecast System (UFS) into Analysis Ready, Cloud Optimized (ARCO; Abernathey et al., (2021)) format. However, the package now pulls data from a number of non-UFS sources, including GFS/GEFS before UFS was created, and even ECMWF's ERA5 dataset.\n",
    "\n",
    "To learn more about ufs2arco, check out the documentation: https://ufs2arco.readthedocs.io/en/latest/index.html\n",
    "\n",
    "We are going to create the following dataset:\n",
    "- NOAA Replay Reanalysis\n",
    "- 3-hourly\n",
    "- Training data dates: 2022-01-01T00 - 2022-01-02T21\n",
    "- Validation data dates: 2022-01-03T00 - 2022-01-04T21\n",
    "- 1-degree global resolution\n",
    "\n",
    "For the purposes of running this notebook, we will not be creating a test set.\n",
    "\n",
    "While this cell is running, go into the `global-eagle/examples/getting_started/colab_notebook_demo/data` folder and look at `logs/logs.serial.out`. This will provide more insight into the dataset creation. Additionally, open `global-eagle/examples/getting_started/colab_notebook_demo/data/replay.yaml` to see all configurations related to data preprocessing.\n",
    "\n",
    "Note: ufs2arco has the capability to parallelize data loading with MPI. We are not taking full advantage of that capability in this notebook. If you wish to use that capability, go see ufs2arco documentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b12fc4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!ufs2arco /content/global-eagle/examples/getting_started/colab_notebook_demo/data/replay.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c91856",
   "metadata": {},
   "source": [
    "After the dataset has completed, let's view it!\n",
    "\n",
    "You will notice that this format looks different than a \"typical\" gridded netcdf or zarr file. Take note of the flattened grid, statistics, nan flag, and attributes. These important details make the dataset ready to be used within a ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3045f3b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "ufs2arco_ds = xr.open_dataset(\"global-eagle/examples/getting_started/colab_notebook_demo/data/replay.zarr\")\n",
    "ufs2arco_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d7069d",
   "metadata": {},
   "source": [
    "### Step 3: Train a model with anemoi-core modules\n",
    "\n",
    "Runtime: 4 minutes\n",
    "\n",
    "We train a graph-based model with the `anemoi-core` modules from the European Centre for Medium-Range Weather Forecasts (ECMWF). The modules include the following:\n",
    "- `anemoi-graphs`: https://anemoi.readthedocs.io/projects/graphs/en/latest/\n",
    "- `anemoi-training`: https://anemoi.readthedocs.io/projects/training/en/latest/\n",
    "- `anemoi-models`: https://anemoi.readthedocs.io/projects/models/en/latest/\n",
    "\n",
    "Training is executed using `anemoi-training`.\n",
    "\n",
    "While training is running, go to the `global-eagle/examples/getting_started/colab_notebook_demo/train/training-output` folder. You will see folders containing checkpoints and plots from your run.\n",
    "\n",
    "We will use the following configurations to train the model:\n",
    "- Model task: Deterministic Forecasting (GraphForecaster)\n",
    "- Model type: Graph Transformer Neural Network\n",
    "- Graph: multi_scale encoder-processor-decoder configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd64dc25",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"ANEMOI_BASE_SEED\"] = \"42\"\n",
    "os.environ[\"SLURM_JOB_ID\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c94c685",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%cd global-eagle/examples/getting_started/colab_notebook_demo/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63903b02",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!anemoi-training train --config-name=config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fcff4e",
   "metadata": {},
   "source": [
    "### Step 4: Create a forecast with anemoi-inference\n",
    "\n",
    "Runtime: 20 seconds\n",
    "\n",
    "Documentation: https://anemoi.readthedocs.io/projects/inference/en/latest/\n",
    "\n",
    "Next, we will run inference using `anemoi-inference`. We will create a 48 hour forecast from 01/03/2022 0Z to 01/04/2022 21Z using a checkpoint from the model we just trained.\n",
    "\n",
    "Before executing inference, you will have to run the first cell to update the inference_yaml with your run_id. This ID is unique to every anemoi-training run. It will update every time you train a model. The first cell updates the yaml to ensure you run inference with the checkpoints you just saved during training. After that, you will be able to successfully run inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e7b4f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_folder = \"/content/global-eagle/examples/getting_started/colab_notebook_demo/train/training-output/checkpoint/\"\n",
    "files = os.listdir(checkpoint_folder)\n",
    "run_id = files[0]\n",
    "\n",
    "inference_yaml_path = \"/content/global-eagle/examples/getting_started/colab_notebook_demo/inference/inference_config.yaml\"\n",
    "with open(inference_yaml_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "with open(inference_yaml_path, \"w\") as f:\n",
    "    for line in lines:\n",
    "        if line.strip().startswith(\"checkpoint\"):\n",
    "            f.write(f\"checkpoint: ../train/training-output/checkpoint/{run_id}/inference-last.ckpt\")\n",
    "        else:\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc24e58",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%cd /content/global-eagle/examples/getting_started/colab_notebook_demo/inference/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b8f922",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!anemoi-inference run inference_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b905dc4e",
   "metadata": {},
   "source": [
    "View inference\n",
    "\n",
    "Feel free to change the variable or forecast hour that you plot! You can see variable options within the opened dataset below. Simply change the `variable_to_plot` or `fhr_index` variables to update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a55f1f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "ds = xr.open_dataset(\"2022-01-03T00.nc\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa416743",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "variable_to_plot = \"tmp2m\"\n",
    "fhr_index = 1\n",
    "\n",
    "var = ds[variable_to_plot].isel(time=fhr_index).values\n",
    "lat = ds['latitude'].values\n",
    "lon = ds['longitude'].values\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(lon, lat, c=var, s=10, marker='s', cmap='coolwarm')\n",
    "plt.colorbar(label=variable_to_plot)\n",
    "plt.title(f'{variable_to_plot} at {ds[\"time\"][fhr_index].values}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c0e1c5",
   "metadata": {},
   "source": [
    "Postprocess inference\n",
    "\n",
    "We perform some postprocessing to ensure that the output will work with the wxvx framework for verification. This includes making the data 2D or 3D again, and adding necessary attributes required by wxvx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b0d045",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!python postprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22b2d08",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "ds_post = xr.open_dataset(\"2022-01-03T00_postprocessed.nc\")\n",
    "ds_post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2180c7",
   "metadata": {},
   "source": [
    "Consider locally saving this final postprocessed netcdf file. This ensures that if you get disconnceted from this runtime, you can go run wxvx at a later time without having to rerun the whole notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61a1030",
   "metadata": {},
   "source": [
    "### Step 5: Verify the forecast against GFS with wxvx\n",
    "\n",
    "Runtime: 4 minutes\n",
    "\n",
    "`wxvx` is a workflow tool for verifying weather models. It leverages `uwtools` to drive `MET`. We are going to run grid-to-grid verification. Verification against observations is currently under development (coming soon!)\n",
    "\n",
    "First, Google Colab does not automatically come with Conda, so we have to install it. We will then run wxvx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be932852",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9890059c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!conda create -y -n wxvx -c ufs-community -c paul.madden wxvx python=3.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793aabc5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%cd global-eagle/examples/getting_started/colab_notebook_demo/verification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022b95a2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MPLBACKEND\"] = \"agg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653ddc60",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!conda run -n wxvx wxvx -c wxvx_config.yaml -t plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12a9d2",
   "metadata": {},
   "source": [
    "Now go to `global-eagle/examples/getting_started/colab_notebook_demo/verification/run/plots/20220103/00` and open some plots comparing (RMSE and ME) our model vs. GFS for numerous variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e61a1e",
   "metadata": {},
   "source": [
    "### Congrats! You made it through the entire `ufs2arco` + `anemoi` + `wxvx` pipeline!\n",
    "\n",
    "Now, feel free to go and change configurations yourself to test various set-ups and learn more about how these modules work. We have provided a few options below!\n",
    "\n",
    "Additional provided tests with `ufs2arco`:\n",
    "- We have provided a `gfs.yaml` in that folder if you wish to try loading in GFS data. To do this, simply replace the replay.yaml within your ufs2arco command with gfs.yaml.\n",
    "\n",
    "Additional provided tests with `anemoi-core` modules during training:\n",
    "- Try testing the use of a `gnn` instead of a `graph transformer`. To try this, switch line 8 of `config.yaml` to say `gnn` instead of `graphtransformer`. We have not provided the `transformer` option because that option requires the `flash-attn` library which cannot run on a T4 GPU.\n",
    "\n",
    "##### If you have any questions, feel free to contact EPIC at support.epic@noaa.gov\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
